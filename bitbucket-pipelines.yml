# This is an example Starter pipeline configuration for GCP Inventory
# Use a skeleton to build, test and deploy using manual and parallel steps
# -----
# You can specify a custom docker image from Docker Hub as your build environment.

# Use Google Cloud SDK image to avoid installing it manually
# 'slim' variant is smaller but still Debian-based (good for Python wheels)
image: google/cloud-sdk:slim

pipelines:
  default:
    - step:
        name: GCP Inventory Generation
        caches:
          - pip
        script:
          # 1. Setup: Define a unique folder for this build to prevent conflicts
          - export WORK_DIR="/tmp/gcp-inventory-$BITBUCKET_BUILD_NUMBER"
          - echo "Setting up workspace at $WORK_DIR"
          - mkdir -p $WORK_DIR

          # 2. Copy the specific files from Repo to Runner
          # We need the requirements and the main source file
          - cp -r install_and_run.sh requirements.txt inventory.py $WORK_DIR/
          - cd $WORK_DIR

          # 3. Permissions: Make the script executable
          - chmod +x install_and_run.sh

          # 4. Authentication: Authenticate with Service Account
          # Decode the Base64 key from the environment variable GCP_SA_KEY_BASE64
          - echo "$GCP_SA_KEY_BASE64" | base64 -d > gcp-inventory-sa-key.json
          - gcloud auth activate-service-account --key-file=gcp-inventory-sa-key.json
          - export GOOGLE_APPLICATION_CREDENTIALS=$(pwd)/gcp-inventory-sa-key.json
          
          # 5. Execution: Run the Inventory Logic
          # It expects two arguments: Project IDs (comma-separated) and Bucket Name.
          # Ensure GCP_PROJECT_ID and GCS_BUCKET_NAME are defined in Repository variables.
          - ./install_and_run.sh "$GCP_PROJECT_ID" "$GCS_BUCKET_NAME"

          # 6. Cleanup: Remove the temporary folder
          - cd ..
          - rm -rf $WORK_DIR
          - echo "Cleanup complete."
